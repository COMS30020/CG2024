<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>04 Wireframes and Rasterising</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">04 Wireframes and Rasterising</h1>
</header>
<h2 id="wireframes-and-rasterising">Wireframes and Rasterising</h2>
<h3
id="weekly-briefing"><a href="https://www.ole.bris.ac.uk/webapps/blackboard/content/launchLink.jsp?course_id=_260093_1&tool_id=_5824_1&tool_type=TOOL&mode=cpview&mode=reset" target="_blank">
Weekly Briefing <img src="../../resources/icons/briefing.png" />
</a></h3>
<h3 id="task-1-introduction">Task 1: Introduction</h3>
<p><a href='01%20Introduction/slides/segment-1.pdf' target='_blank'>
<img src="../../resources/icons/slides.png" /> </a>
<a href='01%20Introduction/audio/segment-1.mp4' target='_blank'> <img
src="../../resources/icons/audio.png" /> </a></p>
<p>This is the workbook we’ve all be waiting for - we finally start to
work in 3 dimensions !</p>
<p>Our fundamental objective for this workbook (in fact the rest of the
unit !) is to take a 3D model (specified as a bunch of vertex positions
in 3 dimensional space) and attempt to render them as a 2 dimensional
image on the screen. Although it might seem like there are a lot of
complex tasks in this workbook, many of them make good use of the 2D
drawing functions that you have previously written. For this reason,
such tasks can be quickly achieved by just calling existing functions in
your code.</p>
<p>Rendering a 3D model onto a 2D canvas requires us to take a
particular perspective/viewpoint on that 3D model. This is only to be
expected - a scene will look different depending on where you view it
from. To aid in our discussion, we introduce the concept of a
<em>camera</em>. Take a look at the slides and audio narration linked to
above for a discussion of this concept. If you did not attend the
briefing lecture session for this workbook, you should watch the video
recording of this now (link at the top of this document) since it
contains important background information about this week’s tasks.</p>
<p>To help you in completing this workbook, you have been provided with
a <code>ModelTriangle</code> class (found in the <code>libs/sdw</code>
folder). This class represents a triangular plane in 3D space and
contains attributes that hold the positions of its three vertices, as
well as the colour of the triangle’s surface. As usual, this class
overrides the <code>&lt;&lt;</code> operator so that you can direct it
to <code>cout</code> for debugging purposes.</p>
<p><strong>Hints &amp; Tips:</strong><br />
<strong>Important</strong>: Make sure you read the “Hints and Tips” at
the end of each section before attempting that task.<br />
This might save you a lot of time and anguish !</p>
<h1 id="section"></h1>
<h3 id="task-2-geometry-files">Task 2: Geometry Files</h3>
<p><a href='02%20Geometry%20Files/slides/segment-1.pdf' target='_blank'>
<img src="../../resources/icons/slides.png" /> </a>
<a href='02%20Geometry%20Files/audio/segment-1.mp4' target='_blank'>
<img src="../../resources/icons/audio.png" /> </a></p>
<p>The first implementation task in this practical is to write a
low-level file reader that can parse in 3D models from numerical data
files. Although the focus of this unit is NOT on grammars and parsing,
understanding how models can be stored in data files provides us with
valuable insight into the essential characteristics of 3D geometry and
material properties. View the slides and audio narration above to find
out about the OBJ file format.</p>
<p>Once you are happy with the concepts above, write a function that
reads in <a href="models/cornell-box.obj" target="_blank">this sample
OBJ geometry file</a> and use the data it contains to populate a vector
of <code>ModelTriangles</code>. At this stage, you should only focus on
reading in vertices (lines that being with a <code>v</code>) and
faces/facets (lines that being with an <code>f</code>). We will deal
with the other types of line later on in this workbook. Be careful -
remember that vertices in OBJ files are indexed from 1 (whereas vectors
are indexed from 0).</p>
<p>Once the sample geometry file has been loaded, loop through all of
the <code>ModelTriangles</code> in your populated vector and print them
out, (you should be able to direct <code>ModelTriangles</code> to
<code>cout</code> !) Open the OBJ file with a text editor and manually
check your printout against a couple of triangles - just to make sure
the data has been loaded correctly.</p>
<p>Add a scaling factor (float) parameter to your OBJ loading function
that scales the position of all vertices at the point at which they are
read in from the file. This is done in order to adjust the size of the
model when it is loaded in (different models might be created to
different scales !). This is to avoid the “bunnyzilla” problem
illustrated in the image below - here two models have been loaded in
from separate files, but the model of the rabbit was originally created
with an unusually large size. Using your scaling factor it would be
possible to adjust the different models to a consistent size.</p>
<p><img src="02%20Geometry%20Files/images/giant-bunny.png" /></p>
<p><strong>Hints &amp; Tips:</strong><br />
To read in the data from the geometry file, you might like to make use
of an
<a href="https://www.w3schools.com/cpp/cpp_files.asp" target="_blank">ifstream</a>
and make calls to the <code>getline</code> function. A
<code>split</code> function has also been provided (see the
<code>Utils</code> class in the <code>libs/sdw</code> folder) to make
tokenising of lines easier. But it is really up to you how you choose to
parse in the files.</p>
<p>Just to reiterate - this unit is about graphical rendering, not
grammars and parsing. As such you should keep your OBJ parser as simple
as possible. It is important to remember that we will NOT be testing the
robustness of your parser by throwing it challenging or malformed
geometry files. Only implement the aspects of the OBJ file format that
you need to complete the tasks at hand. Your parser only needs to be
“good enough” to load in the sample OBJ geometry file provided above
(and any other models that YOU choose to use).</p>
<p>When loading in the Cornell Box model, we suggest you use a scaling
factor of 0.35 since this will bring the all dimensions of the vertices
within the range of -1.0 to 1.0 (which seems sensible and logical).</p>
<h1 id="section-1"></h1>
<h3 id="task-3-material-files">Task 3: Material Files</h3>
<p><a href='03%20Material%20Files/slides/segment-1.pdf' target='_blank'>
<img src="../../resources/icons/slides.png" /> </a>
<a href='03%20Material%20Files/audio/segment-1.mp4' target='_blank'>
<img src="../../resources/icons/audio.png" /> </a></p>
<p>Clearly positional information is important, but how do we know what
the model surfaces should actually <em>look</em> like ? When working
with OBJ files, this kind of aesthetic information is stored in a
separate “materials” file. Your next job is therefore to write a
function to read in
<a href="models/cornell-box.mtl" target="_blank">this sample OBJ
material file</a> and populate a “palette” of colours. You can use the
previously encountered <code>Colour</code> class for this purpose (this
is where the <code>name</code> attribute now becomes useful).</p>
<p>Once you have successfully loaded in the materials file, add some
additional code to your geometry parsing function (from the previous
task) so that it accesses your colour palette when it encounters a
reference to a material (lines that begin with <code>usemtl</code> in
the OBJ geometry file). It makes sense to read in the materials file
<em>before</em> you read in the geometry file - this is so you already
have the colours stored in a data structure when you encounter
references to them in the geometry file. Note that
<code>ModelTriangle</code> objects have a <code>colour</code> attribute
that can be used to store a reference to the colour of triangles.</p>
<p><strong>Hints &amp; Tips:</strong><br />
You could just use a vector to store the palette of colours or, if you
want something a bit fancier, use a hashmap or hashtable for more
efficient colour lookup (using the name of the colour as a key).</p>
<h1 id="section-2"></h1>
<h3 id="task-4-projection-in-theory">Task 4: Projection in Theory</h3>
<p><a href='04%20Projection%20in%20Theory/slides/segment-1.pdf' target='_blank'>
<img src="../../resources/icons/slides.png" /> </a>
<a href='04%20Projection%20in%20Theory/audio/segment-1.mp4' target='_blank'>
<img src="../../resources/icons/audio.png" /> </a>
<a href='04%20Projection%20in%20Theory/animation/segment-1.mp4' target='_blank'>
<img src="../../resources/icons/animation.png" /> </a></p>
<p>Now that we have geometry and material data loaded, we can attempt to
render a graphical representation of the sample model onto the SDL
canvas - or as it is often called in 3D rendering: the “image plane”.
This process is a little complicated, so we have split the job into two
tasks - the “understanding” (this section) and the “doing” (the next
section). Make sure you understand the theory first, before trying to
implement the concepts. First watch just the animated video linked to
above (the last of the blue buttons) for a high-level description of
this process.</p>
<p>As you might imagine, calculating the projected position of a vertex
on the image plane will involve the <em>(x,y,z)</em> position of the
vertex in 3D space. The position of the camera and the distance between
the camera and image plane will also have an effect on the position that
each vertex will appear on that image plane. Everything is based around
the notion of similar triangles - take a look at the slides and narrated
audio for a lower-level explanation of how this works. Note that we
illustrate the process in two dimensions (because it is a lot easier to
show on a powerpoint slide), however the concept “scales” to three
dimensions (we just need to do the same thing in x as we did in y).</p>
<p>When rendering our sample model, there is the additional complexity
that the origin of the Cornell Box model is in the centre of the “room”,
whereas the origin of the image plane is in the top left hand corner
(due to the way the SDL coordinate system works). When rendering a scene
onto the image plane, what we really want is for the centre of the room
to appear in the <em>centre</em> of the image plane. To achieve this, we
need to shift the calculated position of each vertex <em>before</em> we
draw it onto the SDL window. If we shift everything to the
<em>right</em> by half the width of the image plane and <em>down</em> by
half the height of the image plane, the render will appear in the middle
of the SDL window (as shown in the right-hand image below). If we didn’t
do this, the centre of the room would appear in the top left corner of
the drawing window (as shown in the left-hand image below).</p>
<p><img src="04%20Projection%20in%20Theory/images/aligned.jpg" /></p>
<h1 id="section-3"></h1>
<h3 id="task-5-projection-in-practice">Task 5: Projection in
Practice</h3>
<p>In the previous task, we considered projection onto the image plane
abstractly and in theory. In this task we explore how this projection is
actually achieved and then go on to implement it in code. You should
start by adding a new function to your project called
<code>projectVertexOntoCanvasPoint</code> which takes in the following
three parameters:</p>
<ul>
<li><code>cameraPosition</code>: the location of the camera in 3
dimensional space (passed in as a <code>vec3</code>)</li>
<li><code>focalLength</code>: the distance from camera to the image
plane/canvas</li>
<li><code>vertexPosition</code>: the 3 dimensional position of a single
vertex (passed in as a <code>vec3</code>)</li>
</ul>
<p>Add code to this new method to calculate and return the 2D
<code>CanvasPoint</code> position at which the model vertex should be
projected onto the image plane. For a particular vertex, that has a
position in 3D space of <em>(x, y, z)</em> we can calculate its position
on the image plane <em>(u, v)</em> using the two formulae provided below
(where <em>f</em> is focal length, <em>W</em> and <em>H</em> are the
width and height of the image plane). For reference, the
<code>W/2</code> and <code>H/2</code> elements are the shift in origin
used to align the centre of the room with the centre of the image plane
(as shown in the diagram from the previous task).</p>
<p><img
src="05%20Projection%20in%20Practice/images/transpose-to-canvas.png" /></p>
<p><strong>Hints &amp; Tips:</strong><br />
Note that these formulae assume the model vertices are in the <em>camera
coordinate system</em> (i.e. the camera is the origin and x, y and z are
distances relative to the camera position). The vertices in the OBJ
model are however all in the <em>model coordinate system</em>
(i.e. relative to the centre of the room). As a result of this, you will
need to do a transposition in order to convert positions from one
coordinate system to the other. You will need to get used to such
transpositions in this unit, there are numerous origins we have to cope
with !</p>
<p>In the interests of consistency, you should adopt the convention that
positive z is “out of the screen” (towards you) and negative z is “into
the screen” (away from you). Things can get really tricky if different
students use different z directions !</p>
<p>A question that is often asked is “where should I position my camera
?”. A good starting point is to place your camera centred in the x and y
dimensions, but “stepped back” a bit in the z (by 4.0). Your initial
camera position should therefore be <code>(0.0, 0.0, 4.0)</code> - you
should store this position in a <code>vec3</code> variable (since this
will make calculations a lot easier later on).</p>
<p>As for the focal length, a good distance to use is <code>2.0</code>.
Assuming that you followed the above advice about camera position, this
value will position the image plane halfway between the camera and the
centre of the model. Strange things will happen if you position the
image plane behind the model, or even behind the camera !</p>
<p>It is important to note that focal length is a “pure” distance
(i.e. it does not have a direction). You can think of the image plane as
being “attached” to the camera. No matter where the camera is or what
direction it is pointing, the image plane will remain centred in the
camera field of view, always at the same distance from it (the focal
length).</p>
<h1 id="section-4"></h1>
<h3 id="task-6-pointcloud-render">Task 6: Pointcloud Render</h3>
<p>Now that we have a function that can map/project a 3D vertex onto a
2D canvas point, we are in a position where we can actually attempt to
display a render of a 3D model !</p>
<p>We start with the simplest render possible - a “pointcloud”.</p>
<p>Using your <code>projectVertexOntoCanvasPoint</code> function,
calculate the image plane positions of each vertex of the previously
loaded Cornell Box. Due to the size of the model (vertex positions in
the range -1.0 to 1.0) the positions of the calculated points on the
image plane will all be very close to the centre of the screen. To solve
this problem, you should apply an “image plane scaling” to resize the
coordinates so that they fit nicely onto the drawing window. A
multiplier of 160 seems to be a good initial value for this scaling
factor, but feel free to experiment with this value until to get a
render that fits well onto the image plane. Make sure you scale x and y
by the same amount (in order to maintain the aspect ratio of the point
cloud).</p>
<p>If you draw a single white pixel at each projected image plane
position, you should end up with a pointcloud render that looks like the
image shown below. Although very minimalistic, with a bit of imagination
you can see the room corners and boxes from the sample model.</p>
<p><img src="06%20Pointcloud%20Render/images/pointcloud.png" /></p>
<p><strong>Hints &amp; Tips:</strong><br />
You might find (depending on how you write your code) that your render
is flipped upside-down. Any ideas why this might be ? Any thoughts about
how to fix this problem ?</p>
<h1 id="section-5"></h1>
<h3 id="task-7-wireframe-render">Task 7: Wireframe Render</h3>
<p>Although we have rendered a representation of our model onto the
image plane, this render is basic (and a little hard to check for
correctness). Our next step is to draw a “wireframe” render of the
Cornell Box model onto the image plane - this makes it a lot easier to
view the structure of the model. The basic principle is to create a 2D
<code>CanvasTriangle</code> for each 3D <code>ModelTriangle</code> that
was read in from the OBJ geometry file. To achieve this, you should make
use of your <code>projectVertexOntoCanvasPoint</code> function in order
to convert 3D model vertices into 2D canvas positions.</p>
<p>In order to draw the 2D canvas triangles onto the image plane, you
can make use of your 2D stroked triangle drawing function (that you
wrote for the previous workbook). You see - the time spent working in 2D
was useful after all ! If everything works correctly, you should end up
with a wireframe render that looks something like the image below.</p>
<p><img src="07%20Wireframe%20Render/images/wireframe.jpg" /></p>
<p><strong>Hints &amp; Tips:</strong><br />
You might find (depending on how you write your code) that your render
is flipped upside-down. Any ideas why this might be ? Any thoughts about
how to fix this problem ?</p>
<h1 id="section-6"></h1>
<h3 id="task-8-rasterised-render">Task 8: Rasterised Render</h3>
<p>After you have successfully rendered a wireframe of the model, the
next step is to fill those triangles ! This is going to be a lot easier
than you might think !!! The image pane is 2D and you have already
written a 2D “filled triangle” rasterising function in a previous
workbook. Using the correct colours to fill the triangles, call this
function in order to draw the Cornell Box model. The end product should
look something like the image below (take a look at this week’s briefing
video for some reassurance - if your render looks <em>similar</em>, but
slightly different).</p>
<p><img src="08%20Rasterised%20Render/images/overlap.jpg" /></p>
<h1 id="section-7"></h1>
<h3 id="task-9-occlusion-problem">Task 9: Occlusion Problem</h3>
<p><a href='09%20Occlusion%20Problem/slides/segment-1.pdf' target='_blank'>
<img src="../../resources/icons/slides.png" /> </a>
<a href='09%20Occlusion%20Problem/audio/segment-1.mp4' target='_blank'>
<img src="../../resources/icons/audio.png" /> </a>
<a href='09%20Occlusion%20Problem/animation/segment-1.mp4' target='_blank'>
<img src="../../resources/icons/animation.png" /> </a></p>
<p>You might have noticed a problem with the above image ? Depending on
the order in which the triangles appear in an OBJ file (and the order
you store them in your code), rendered triangles can sometimes overlap
each other inappropriately. For example, in the image shown in the
previous task, the blue box is shown overlapping the red one - which is
not the case in the true model. View the slides and narrated audio above
to understand what causes this problem and what we can do to solve
it.</p>
<p>In your code, create a 2D array of floats (the dimensions of the
array being the same width and height as the image plane / drawing
window). You are going to use this array to keep track of the z depth of
the model element drawn for each pixel on the screen. The data stored in
this “depth buffer” will allow us to decide what colour to paint a pixel
when there is contention. It is important that we use the
<em><strong>inverse</strong></em> of the z depth (i.e. <code>1/Z</code>)
of the model element represented. This is to take into account the
effects of perspective in the rendered image (this will be explored in
more detail in a later session).</p>
<p>When filling the depth buffer, you need to calculate the inverse z
depth of specific points on the surface of a triangle. It is worth
observing that you do not actually <em>have</em> depth information for
all these points. These depths can however be calculated by
interpolating the depths of known points (i.e. the vertices). Watch the
animation linked to at the top of this section for help on achieving
this.</p>
<p>Use your depth buffer in your drawing functions to help decide what
colour to set a particular image plane pixel. In some cases the first
colour you draw a pixel will be the correct one, in other cases this
colour will need to be overwritten by a subsequent colour. If your depth
buffer is operating correctly, you should see something like the
left-hand image shown below. The two right-hand images show the
difference between interpolating <code>Z</code> and interpolating
<code>1/Z</code>.</p>
<p><img src="09%20Occlusion%20Problem/images/combined.jpg" /></p>
<p><strong>Hints &amp; Tips:</strong><br />
Note that the <code>CanvasPoint</code> class has a <code>depth</code>
attribute that you might like to use to help you keep track of the
inverse z depth of each vertex of a <code>CanvasTriangle</code>.</p>
<p>When you create the depth buffer array you should first initialise
every element to zero. This signifies that at the start (before
rendering the scene takes place) that there is no model element
represented in any pixel position. Another way to think of this is that
this value represents <code>1/Z</code> where Z is infinity, so that any
model element that is subsequently encountered is sure to be closer.</p>
<p>Because we are using the inverse of the z depth, we must remember
that the colour a particular pixel should be drawn must be the model
element with the <em>largest</em> <code>1/Z</code>. Consider the diagram
below for example - the pixel on the image plane through which the
dotted line passes should clearly be drawn red (since the red triangle
is the closest to the camera/eye). However <code>1/Z</code> for the red
triangle (0.5) is <em>greater</em> than that of the blue triangle
(0.25). The <em>largest</em> <code>1/Z</code> always wins !</p>
<p><img src="z-depth.png" /></p>
<h1 id="section-8"></h1>
<h3 id="end-of-workbook">End of workbook</h3>
</body>
</html>
